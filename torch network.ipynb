{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp37-cp37m-win_amd64.whl (162.6 MB)\n",
      "Requirement already satisfied: typing-extensions in d:\\programs\\lib\\site-packages (from torch) (4.5.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1537403456,        440, 1537403440],\n",
      "        [       440, 1537405376,        440]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.IntTensor(2, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронная сеть в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Любая нейронная сеть в PyTorch реализуется как класс. У этого класса есть важная особенность: он должен быть унаследован от класса `torch.nn.Module`. \n",
    "<br>**Наследование** — это процесс, когда один класс использует атрибуты и методы другого класса как свои собственные, а также расширяет его возможности. Класс, чьи свойства и методы наследуются, называют «суперклассом» или «родителем», а наследующий класс — «подклассом» или «потомком».\n",
    "## Пример 1\n",
    "«Точка» — умная колонка. Она умеет изменять громкость от 0 до 10 и обрабатывать запросы пользователей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TochkaSmartSpeaker:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.volume = 5\n",
    "\n",
    "    def change_volume(self, volume):\n",
    "        if volume < 0:\n",
    "            self.volume = 0\n",
    "        elif volume > 10:\n",
    "            self.volume = 10\n",
    "        else:\n",
    "            self.volume = volume\n",
    "\n",
    "    def respond(self, request):\n",
    "        return 'I am processing your request: ' + request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разработчики колонки придумали второе поколение «Точки» на основе первого — добавили кнопку отключения звука. Теперь, если звук отключён, то запросы не обрабатываются. Чтобы эта функция заработала, нужно дополнить код. Сначала добавим при инициализации атрибут mute — он будет отвечать за отключение звука:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TochkaSmartSpeaker2ndGeneration(TochkaSmartSpeaker):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.is_mute = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методом `super()` вызываем соответствующий метод у класса-родителя. В нашем случае, вызывая `super().__init__()`, мы инициализируем поле `volume`. В классе-потомке можно безболезненно создавать дополнительные методы, которые будут включать и выключать звук:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TochkaSmartSpeaker2ndGeneration(TochkaSmartSpeaker):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.is_mute = False\n",
    "\n",
    "    def mute(self):\n",
    "        self.is_mute = True\n",
    "\n",
    "    def unmute(self):\n",
    "        self.is_mute = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось написать корректную обработку запросов. Когда звук не выключен, колонка второго поколения работает как предыдущая модель. При выключенном звуке в ответ на запрос возвращается пустая строка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TochkaSmartSpeaker2ndGeneration(TochkaSmartSpeaker):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.is_mute = False\n",
    "\n",
    "    def mute(self):\n",
    "        self.is_mute = True\n",
    "\n",
    "    def unmute(self):\n",
    "        self.is_mute = False\n",
    "\n",
    "    def respond(self, request):\n",
    "        return \"\" if self.is_mute else super().respond(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наследование позволяет использовать неизменное поведение — в примере это регулировка громкости. Если поведение изменилось (появилась кнопка выключения), мы не переписываем весь код, а сохраняем неизменную часть и дописываем то, что поменялось.<br><br>\n",
    "Вернёмся к задаче построения нейронной сети. В первую очередь нужно создать класс-наследник класса `nn.Module`. Без наследования написать нейронную сеть в PyTorch не получится, вам придётся переписывать очень много различных методов, необходимых для нормального функционирования сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем нужно определить архитектуру нейронной сети. Данные в `PyTorch` представлены тензорами. \n",
    "Полносвязный слой называется `Linear` и находится в библиотеке `torch.nn`. У него есть два параметра: `in_features` — количество нейронов на входе слоя, `out_features` — число нейронов на выходе.\n",
    "Но в этом полносвязном слое нет функции активации, её нужно применять отдельно. Функции активации находятся в той же библиотеке `torch.nn`. Посмотреть, какие функции активации доступны, можно в документации: https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity. \n",
    "## Пример 2\n",
    "Инициализируем полносвязную нейронную сеть из двух полносвязных слоёв. После первого слоя применим сигмоидную функцию активации, а после второго — ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in_neurons, n_hidden_neurons, n_out_neurons):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in_neurons, n_hidden_neurons)\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(n_hidden_neurons, n_out_neurons)        \n",
    "        self.act2 = nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы объявили архитектуру нейронной сети, но не то, как она будет работать. Теперь необходимо переопределить метод `forward`. Он принимает на вход тензор и возвращает его на выходе, после прохождения через нейронную сеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in_neurons, n_hidden_neurons, n_out_neurons):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in_neurons, n_hidden_neurons)\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(n_hidden_neurons, n_out_neurons)        \n",
    "        self.act2 = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось объявить объект класса — и можно запускать нейронную сеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(5, 4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы прогнать тензор через сеть, нужно вызвать метод `forward`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-48c8df67c677>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'input_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "output_tensor = net.forward(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 1\n",
    "Добавьте в нейронную сеть из примера второй скрытый слой. Для второго скрытого слоя примените гиперболический тангенс `nn.Tanh()` в качестве функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in_neurons, n_hidden_neurons_1, n_hidden_neurons_2, n_out_neurons):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in_neurons, n_hidden_neurons_1)\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(n_hidden_neurons_1, n_hidden_neurons_2) # Ваш код\n",
    "        self.act2 = nn.Tanh() # Ваш код\n",
    "        self.fc3 = nn.Linear(n_hidden_neurons_2, n_out_neurons)\t\t\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)# Ваш код\n",
    "        x = self.fc3(x)\n",
    "        x = self.act3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2\n",
    "Инициализируйте нейронную сеть, состоящую из трёх входных нейронов, пяти нейронов в первом скрытом слое, трёх нейронов во втором скрытом слое и одного нейрона в выходном слое. Создайте тензор из значений [1, 44, -7] и прогоните тензор через сеть. Результат выведите на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in_neurons, n_hidden_neurons_1, \n",
    "\t\t\t\t\t\t\t\t\tn_hidden_neurons_2, n_out_neurons):\n",
    "        super(Net, self).__init__()\n",
    "\t\t\t\n",
    "        self.fc1 = nn.Linear(n_in_neurons, n_hidden_neurons_1)\n",
    "        self.act1 = nn.Sigmoid()\n",
    "\t\n",
    "        self.fc2 = nn.Linear(n_hidden_neurons_1, n_hidden_neurons_2)\n",
    "        self.act2 = nn.Tanh()\n",
    "\n",
    "        self.fc3 = nn.Linear(n_hidden_neurons_2, n_out_neurons)\t\t\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "\t\t\t\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "\t\t\t\n",
    "        x = self.fc3(x)\n",
    "        x = self.act3(x)\n",
    "        return x\n",
    "\n",
    "net = Net(3, 5, 3, 1)  # инициализируйте нейронную сеть \n",
    "x = torch.Tensor([1, 44, -7]) # инициализируйте тензор\n",
    "print(net.forward(x)) # выведите результат прогонки тензора "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 3\n",
    "Через нейронную сеть можно прогонять не только отдельные объекты, но и датасеты. Датасеты обрабатываются как тензоры.\n",
    "Создайте нейронную сеть, содержащую два скрытых слоя: первый содержит `n_hidden_neurons_1`, а второй — `n_hidden_neurons_2`.<br>\n",
    "После первого скрытого слоя примените `гиперболический тангенс` в качестве функции активации, после второго — `ReLU`, после выходного слоя — `сигмоиду`.<br>\n",
    "Дополните код прямого распространения в методе `forward()`.<br>\n",
    "Инициализируйте нейронную сеть, состоящую из `десяти входных нейронов`, с`еми нейронов в первом скрытом слое`, `четырёх нейронов во втором скрытом слое` и `одного нейрона в выходном слое`. Создайте `тензор размером 100 на 10`, состоящий из случайных вещественных значений. Прогоните тензор через сеть. Результат выведите на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6067],\n",
      "        [0.6191],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6260],\n",
      "        [0.6059],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6059],\n",
      "        [0.6067],\n",
      "        [0.6059],\n",
      "        [0.6067],\n",
      "        [0.6059],\n",
      "        [0.6059],\n",
      "        [0.6059],\n",
      "        [0.6115],\n",
      "        [0.6059],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6059],\n",
      "        [0.6260],\n",
      "        [0.6069],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6115],\n",
      "        [0.6069],\n",
      "        [0.6067],\n",
      "        [0.6059],\n",
      "        [0.6067],\n",
      "        [0.6115],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6115],\n",
      "        [0.6059],\n",
      "        [0.6260],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6115],\n",
      "        [0.6055],\n",
      "        [0.6260],\n",
      "        [0.6059],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6144],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6191],\n",
      "        [0.6115],\n",
      "        [0.6069],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6069],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6191],\n",
      "        [0.6059],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6191],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6260],\n",
      "        [0.6069],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6191],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6067],\n",
      "        [0.6059],\n",
      "        [0.6260],\n",
      "        [0.6059],\n",
      "        [0.6067],\n",
      "        [0.6059],\n",
      "        [0.6067],\n",
      "        [0.6067]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in_neurons, n_hidden_neurons_1, \n",
    "\t\t\t\t\t\t\t\t\tn_hidden_neurons_2, n_out_neurons):\n",
    "        super(Net, self).__init__()\n",
    "\t\t\t\n",
    "        self.fc1 = nn.Linear(n_in_neurons, n_hidden_neurons_1)\n",
    "        self.act1 = nn.Tanh()\n",
    "\t\n",
    "        self.fc2 = nn.Linear(n_hidden_neurons_1, n_hidden_neurons_2)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(n_hidden_neurons_2, n_out_neurons)\n",
    "        self.act3 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "\t\t\t\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "\t\t\t\n",
    "        x = self.fc3(x)\n",
    "        x = self.act3(x)\n",
    "        return x\n",
    "\n",
    "net = Net(10, 7, 4, 1) # инициализируйте нейронную сеть \n",
    "x = torch.FloatTensor(100, 10) # инициализируйте тензор  \n",
    "print(net.forward(x)) # выведите результат прогонки тензора "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот способ инициализации универсальный, но для простых сетей, таких как полносвязные сети прямого распространения, он выглядит громоздко.\n",
    "# Сети прямого распространения в PyTorch\n",
    "Сети прямого распространения создаются альтернативным способом — с помощью контейнера `Sequential`. Этот контейнер строит сеть прямого распространения на основе заданных слоёв. В `Sequential` уже есть метод `forward`, в котором входной тензор последовательно проходит через все слои.<br>\n",
    "При инициализации объекта класса-контейнера `Sequential` в качестве параметров по очереди передаются слои, которые будут использованы. При этом их порядок изменить уже не получится, нужно будет создавать другой объект.\n",
    "Инициализируем сеть из прошлого урока, используя контейнер `Sequential`. Так она выглядела при создании через `nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in_neurons, n_hidden_neurons, n_out_neurons):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in_neurons, n_hidden_neurons)\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(n_hidden_neurons, n_out_neurons)        \n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так её можно переписать через `Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_in_neurons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4e166c4d389e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m net = nn.Sequential(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_in_neurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden_neurons\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_hidden_neurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_out_neurons\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_in_neurons' is not defined"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(n_in_neurons, n_hidden_neurons),\n",
    "    nn.Sigmoid(), \n",
    "    nn.Linear(n_hidden_neurons, n_out_neurons), \n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы сразу получаем объект класса-контейнера `Sequential`.<br>\n",
    "## Задача 1\n",
    "Перепишите трёхслойную нейронную сеть из задачи 1 прошлого урока: добавьте в нейронную сеть прямого распространения второй скрытый слой. Для второго скрытого слоя примените гиперболический тангенс `nn.Tanh()` в качестве функции активации.\n",
    "Задайте конкретные значения для количества нейронов: во входном — $4$, в первом скрытом — $6$, во втором скрытом — $3$, в выходном слое — $1$.\n",
    "Создайте тензор размером $50$ на $4$, состоящий из случайных вещественных значений, и прогоните тензор через сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.1556],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.1556],\n",
      "        [0.1556],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.1556],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.0317],\n",
      "        [0.1556],\n",
      "        [0.0317],\n",
      "        [0.1556],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "n_in_neurons = 4# Ваш код\n",
    "n_hidden_neurons_1 = 6# Ваш код\n",
    "n_hidden_neurons_2 = 3# Ваш код\n",
    "n_out_neurons = 1# Ваш код\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(n_in_neurons, n_hidden_neurons_1),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(n_hidden_neurons_1, n_hidden_neurons_2),\n",
    "    nn.Tanh(),# Ваш код\n",
    "    nn.Linear(n_hidden_neurons_2, n_out_neurons), \n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "x = torch.Tensor(50, 4) # инициализируйте тензор\n",
    "print(net.forward(x)) # выведите результат прогонки тензора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2\n",
    "Создайте полносвязную нейронную сеть с произвольным числом скрытых слоёв. Количество нейронов в каждом слое задано в списке `n_neurons`. Длина списка не меньше $2$. Каждый элемент списка является числом нейронов в соответствующих слоях. В качестве функций активации для нечётных слоёв используйте сигмоиду `nn.Sigmoid()`, для чётных слоёв — гиперболический тангенс `nn.Tanh()`, входной слой считается нулевым. Для выходного слоя используйте функцию активации `nn.ReLU()`. \n",
    "Создайте тензор размером $50$ на $20$, состоящий из случайных вещественных значений, и прогоните тензор через сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=20, out_features=16, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=16, out_features=12, bias=True)\n",
      "  (3): Sigmoid()\n",
      "  (4): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (5): Tanh()\n",
      "  (6): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (7): Sigmoid()\n",
      "  (8): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (9): Tanh()\n",
      "  (10): Linear(in_features=2, out_features=1, bias=True)\n",
      "  (11): ReLU()\n",
      ")\n",
      "tensor([[0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0290],\n",
      "        [0.0256],\n",
      "        [0.0320],\n",
      "        [0.0305],\n",
      "        [0.0289]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "n_neurons = [20, 16, 12, 8, 4, 2, 1]\n",
    "net_layers = []\n",
    "\n",
    "for i in range(1, len(n_neurons) - 1):\n",
    "\t\tnet_layers.append(nn.Linear(n_neurons[i-1], n_neurons[i])) # добавьте полносвязный слой\n",
    "\t\tif (i+1) % 2 == 0:\n",
    "\t\t\t\tnet_layers.append(nn.Tanh()) # добавьте функцию активации для чётных слоёв\n",
    "\t\telse:\n",
    "\t\t\t\tnet_layers.append(nn.Sigmoid()) # добавьте функцию активации для нечётных слоёв\n",
    "\n",
    "net_layers.append(nn.Linear(n_neurons[-2], n_neurons[-1])) # добавление выходного слоя\n",
    "net_layers.append(nn.ReLU()) #\n",
    "\n",
    "net = nn.Sequential(*net_layers) # такая запись позволяет передавать элементы списка как параметры для инициализации\n",
    "print(net) # вывод архитектуры сети\n",
    "\n",
    "x = torch.Tensor(50, 20) # инициализируйте тензор\n",
    "print(net.forward(x)) # выведите результат прогонки тензора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция потерь\n",
    "$L=L(y, \\hat{y})$ на вход этой функции подают результат вычисления сети для объекта $\\hat{y}$ и известный ответ $y$. На выходе она выдаст разницу между выводом модели и известным ответом.\n",
    "1. Метрика качества нужна, чтобы оценить работу модели на новых для неё данных.\n",
    "2. Функция потерь помогает найти наилучшее значение параметров на тренировочных данных.\n",
    "<br>Как это было с функциями активации, для разных задач подходят разные функции потерь.\n",
    "\n",
    "## Задача регрессии\n",
    "В этой задаче функцией потери может быть среднеквадратичное отклонение (англ. mean squared error, $MSE$) и среднее абсолютное отклонение (англ. mean absolute error, $MAE$). Они выглядят как соответствующие метрики качества:<br>\n",
    "* $MSE(y, \\hat{y})=(y-\\hat{y})^2$\n",
    "* $MAE(y, \\hat{y})=|y-\\hat{y}|$\n",
    "<br>В `PyTorch` эти функции потерь находятся в модуле `torch.nn`: `MSELoss`, `L1Loss`.<br>\n",
    "\n",
    "## Задача бинарной классификации\n",
    "Вместо функции потерь используют бинарную кросс-энтропию (англ. binary cross-entropy, `BCE`):<br>\n",
    "$BCE(y, \\hat{y})=-y log \\hat{y}-(1-y) \\log{(1-\\hat{y}})$\n",
    "<br>Чтобы бинарная кросс-энтропия работала корректно, нужно в последнем слое сети использовать `сигмоиду` в качестве функции активации. Тогда значения предсказаний будут от $0$ до $1$.\n",
    "<br>В `PyTorch` бинарная кросс-энтропия называется `BCELoss` и находится в модуле `torch.nn`.\n",
    "Если по какой-то причине `сигмоида` или другая функция активации в последнем слое не используется, замените `BCELoss` на `BCEWithLogitsLoss`. В последней функции `сигмоида` применяется перед вычислением кросс-энтропии.\n",
    "\n",
    "## Задача многоклассовой классификации\n",
    "Когда нужно определить принадлежность объекта одному из $C$ классов, в роли функции потерь чаще всего применяется обобщение бинарной кросс-энтропии — категориальная кросс-энтропия (англ. categorical cross-entropy, `CCE`):<br>\n",
    "$CCE(y, \\hat{y})=-\\sum{_{j=1} ^ {C}} y_i log \\hat{y}_j$<br>\n",
    "$y_j=1$, если объект принадлежит классу $j$, в противном случае $y_j=0$\n",
    "<br>Для категориальной кросс-энтропии необходимо, чтобы предсказания были от 0 до 1 (как и для бинарной), поэтому в последнем слое нужно применить функцию потерь `Softmax`.<br>\n",
    "Категориальная кросс-энтропия в `PyTorch` находится в модуле torch.nn и называется `CrossEntropyLoss`.<br>\n",
    "### Вычислить функции потерь для набора данных можно двумя способами:\n",
    "1. посчитать среднее арифметическое функции потерь для всех объектов датасета,\n",
    "2. вычислить суммы значений функции потерь для всех объектов датасета.<br>\n",
    "За выбор способа отвечает параметр `reduction`. Он может принимать одно из трёх значений: `mean` — для вычисления среднего арифметического (значение по умолчанию), `sum` — для вычисления суммы, `none` — для вычисления вектора из значений функции потерь для конкретных объектов.\n",
    "\n",
    "## Задача 1\n",
    "Чтобы решить задачу регрессии с одним скрытым слоем, инициализируйте нейронную сеть прямого распространения, состоящую из пяти входных нейронов, трёх нейронов в скрытом слое и одного нейрона в выходном слое. После каждого слоя добавьте функцию активации `ReLU`.<br>\n",
    "Создайте тензор `x` из значений [-0.23, -0.2, 0.31, -0.9, 0.2] и прогоните его через сеть. Результат прогонки сохраните в переменной `y_hat`. Создайте тензор y из значения [0.15], в котором будет храниться целевая переменная.\n",
    "Инициализируйте **функцию потерь** `MAE` и вычислите значение функции потерь для предсказания модели и целевой переменной.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "tensor(0.1402, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "n_in_neurons = 5# число входных нейронов\n",
    "n_hidden_neurons = 3# число нейронов в скрытом слое \n",
    "n_out_neurons = 1# число выходных нейронов \n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(n_in_neurons, n_hidden_neurons)\n",
    "    , nn.ReLU()\n",
    "    , nn.Linear(n_hidden_neurons, n_out_neurons)\n",
    "    , nn.ReLU()\n",
    ")\n",
    "print(0)\n",
    "x = torch.Tensor([-0.23, -0.2, 0.31, -0.9, 0.2])\n",
    "print(1)\n",
    "y = torch.Tensor([0.15])\n",
    "print(2)\n",
    "y_hat = net.forward(x)# прогоните x через нейронную сеть\n",
    "print(3)\n",
    "\n",
    "loss = nn.L1Loss()# инициализируйте функцию потерь MAE\n",
    "print(4)\n",
    "\n",
    "print(loss(y_hat, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2\n",
    "Измените сеть из предыдущей задачи так, чтобы она могла решить задачу `бинарной классификации`. Для этого после выходного слоя замените функцию активации на `сигмоиду`.<br>\n",
    "Создайте тензор x из значений [-0.23, -0.2, 0.31, -0.9, 0.2] и прогоните тензор через сеть. Результат сохраните в переменной `y_hat`. Создайте тензор y из значения [0], в котором будет храниться целевая переменная.<br>\n",
    "Инициализируйте функцию потерь `бинарной кросс-энтропии` и вычислите значение функции потерь для предсказания модели и целевой переменной.<br>\n",
    "Модель на выходе выдаёт вещественное число от 0 до 1 — «вероятность» принадлежности классу 1. Чтобы перевести это число в класс 0 или 1, сравните его с порогом 0.5: если выход модели больше 0.5, должен быть класс 1, в противном случае — 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5508, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "n_in_neurons = 5 \n",
    "n_hidden_neurons = 3\n",
    "n_out_neurons = 1 \n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(n_in_neurons, n_hidden_neurons),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(n_hidden_neurons, n_out_neurons), \n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "x = torch.Tensor([-0.23, -0.2, 0.31, -0.9, 0.2])\n",
    "y = torch.Tensor([0])\n",
    "y_hat = net.forward(x)# прогоните x через нейронную сеть\n",
    "\n",
    "loss = nn.BCELoss()# инициализируйте функцию потерь бинарной кросс-энтропии\n",
    "\n",
    "print(loss(y_hat, y))\n",
    "\n",
    "print(int(y_hat > 0.5 )) # выведите класс, которому принадлежит данный объект, по мнению модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 3\n",
    "Измените сеть из предыдущей задачи так, чтобы она решала задачу `многоклассовой классификации`. Для этого замените число нейронов на выходе из сети на $3$ и после выходного слоя замените функцию активации на `Softmax()`.<br>\n",
    "Создайте тензор x из значений [[-0.23, -0.2, 0.31, -0.9, 0.2], [0.23, 0.2, -0.31, 0.9, -0.2]] и прогоните тензор через сеть. Результат сохраните в переменной `y_hat`. Создайте тензор y из значения [2, 0], в котором будет храниться целевая переменная.<br>\n",
    "Инициализируйте функцию потерь кросс-энтропии и вычислите значение функции потерь для предсказания модели и целевой переменной.<br>\n",
    "Модель на выходе выдаёт «вероятность» принадлежности каждому из классов. Выведите класс методом `argmax(dim=1)`, который для каждого предсказания даёт индекс максимального элемента списка вероятностей — наиболее вероятный класс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\lib\\site-packages\\torch\\nn\\modules\\container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1406, grad_fn=<NllLossBackward0>)\n",
      "tensor([[0.3439, 0.3081, 0.3481],\n",
      "        [0.2370, 0.3807, 0.3823]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "n_in_neurons = 5 \n",
    "n_hidden_neurons = 3\n",
    "n_out_neurons = 3 \n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(n_in_neurons, n_hidden_neurons),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(n_hidden_neurons, n_out_neurons), \n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "x = torch.Tensor([[-0.23, -0.2, 0.31, -0.9, 0.2], [0.23, 0.2, -0.31, 0.9, -0.2]])\n",
    "\n",
    "y = torch.LongTensor([2, 0])\n",
    "\n",
    "y_hat = net.forward(x) # прогоните x через нейронную сеть\n",
    "\n",
    "loss = nn.CrossEntropyLoss()# инициализируйте функцию потерь категориальной кросс-энтропии\n",
    "\n",
    "print(loss(y_hat, y))\n",
    "\n",
    "print(y_hat)\n",
    "print(torch.argmax(y_hat, dim=1) ) # выведите класс, которому принадлежит данный объект, по мнению модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обратное распространение ошибки\n",
    "Градиент показывает направление наискорейшего возрастания функции, антиградиент (отрицательный градиент) — направление наибольшего убывания, формулa для градиентного спуска:<br>\n",
    "$\\omega^{t+1}=\\omega^t+\\Delta{\\omega^t}$<br>\n",
    "$\\Delta{\\omega^t}=-\\eta\\frac{dL}{d\\omega^t}$<br>\n",
    "Где $\\omega^t$, $\\omega^{t+1}$\n",
    "\n",
    "НУЖНО ДОПИСАТЬ ПРО ГРАДИЕНТЫ, ФУНКЦИИ ПОТРЕЬ И Т.П.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка модели\n",
    "Чтобы использовать оптимизатор в обучении модели, сначала его нужно объявить. В первую очередь в оптимизатор передают значения, которые нужно оптимизировать. Если это нейронная сеть, то нужно передать её параметры с помощью метода `parameters()`. Затем можно указать параметры алгоритма оптимизации. Например, скорость обучения — `learning rate`, `lr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a6b2d4eaa10b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Net' is not defined"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "...\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1.0e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорость обучения можно менять. <i>Возьмёте слишком большую — рискуете «проскочить» минимум, слишком маленькую — алгоритм будет работать очень медленно.</i><br>\n",
    "В `PyTorch` оптимизаторы имеют свойство накапливать градиент. Это может повредить обучению, поэтому перед каждым шагом нужно обнулить накопленный градиент методом `zero_grad()`. Затем вычисляются текущие предсказания модели, подсчитывается значение функции потерь. После этого с помощью метода `backward()` вычисляется значение градиента функции потерь и выполняется шаг работы оптимизатора (обновление весов в сети):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b3bd36fb4419>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "\n",
    "preds = net.forward(X) \n",
    "        \n",
    "loss_value = loss(preds, y)\n",
    "loss_value.backward()\n",
    "        \n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще раз последовательность должна быть такая:\n",
    "1. Сделать шаг оптимизации, вызвав метод `step()`.\n",
    "2. Получить предсказания с текущими весами, прогнав признаки через сеть с помощью `forward()`.\n",
    "3. Получить значение функции потерь для предсказаний и истинных значений.\n",
    "4. Вычислить градиенты функции потерь с помощью метода `backward()`.\n",
    "5. Сделать шаг оптимизации, вызвав метод `step()`.\n",
    "# Обучение нейронной сети в PyTorch\n",
    "## Эпоха обучения\n",
    "Нейронные сети обучаются в несколько итераций — эпох. В одной эпохе происходит обнуление градиентов в оптимизаторе, прямое распространение, подсчёт значения функции потерь, вычисление градиента для функции потерь, изменение весов с помощью оптимизатора, вычисление метрики качества. <u>Перед вычислением метрики качества желательно перевести модель в режим предсказания — вызвать метод `eval()`</u>. В этом режиме не будут вычисляться и накапливаться градиенты. Перед вами фрагмент кода обучения сети для одной эпохи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad() # обнуление градиентов\n",
    "\n",
    "preds = net.forward(X_train)  # прямое распространение на обучающих данных\n",
    "        \n",
    "loss_value = loss(preds, y_train) # вычисление значения функции потерь\n",
    "loss_value.backward() # вычисление градиентов\n",
    "        \n",
    "optimizer.step() # один шаг оптимизации весов\n",
    "\n",
    "net.eval() # перевод сети в режим предсказания\n",
    "test_preds = net.forward(X_test) # прямое распространение на тестовых данных\n",
    "accuracy = (test_preds.argmax(dim=1) == y_test).float().mean().data # вычисление доли правильных ответов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Число эпох задаётся перед обучением, оно может быть любым. Такой фрагмент кода обучает сеть за $100$ эпох: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5b9b45977378>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = net.forward(X_train) \n",
    "            \n",
    "    loss_value = loss(preds, y_train)\n",
    "    loss_value.backward()\n",
    "            \n",
    "    optimizer.step()\n",
    "    \n",
    "    net.eval()\n",
    "    test_preds = net.forward(X_test)\n",
    "    accuracy = (test_preds.argmax(dim=1) == y_test).float().mean().data\n",
    "    \n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда метрику качества вычисляют не каждую эпоху, а, например, каждую пятую или десятую:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds = net.forward(X_train) \n",
    "            \n",
    "    loss_value = loss(preds, y_train)\n",
    "    loss_value.backward()\n",
    "            \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        net.eval()\n",
    "        test_preds = net.forward(X_test)\n",
    "        accuracy = (test_preds.argmax(dim=1) == y_test).float().mean().data\n",
    "        \n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Батчи\n",
    "Когда идёт обучение на больших датасетах, не всегда получается загрузить все нужные данные в оперативную память. Поэтому в каждой эпохе нейронная сеть получает данные частями — батчами (`batch`).\n",
    "Как получать батчи — сейчас расскажем. В начале эпохи нужно сгенерировать случайную перестановку объектов обучающей выборки (а точнее, их индексов). Это нужно, чтобы батчи изменялись от эпохи к эпохе, иначе тренировка будет неэффективной. Затем нужно получить индекс текущего батча и сформировать подвыборку из обучающей выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c3cd474f7118>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mnum_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "num_batches = ceil(len(X_train)/batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # случайная перестановка объектов\n",
    "    order = np.random.permutation(len(X_train))\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_index = batch_idx * batch_size\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # получение индексов текущего батча\n",
    "        batch_indexes = order[start_index:start_index+batch_size]\n",
    "        X_batch = X_train[batch_indexes]\n",
    "        y_batch = y_train[batch_indexes]\n",
    "    \n",
    "        preds = net.forward(X_batch) \n",
    "                \n",
    "        loss_value = loss(preds, y_batch)\n",
    "        loss_value.backward()\n",
    "                \n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        net.eval()\n",
    "        test_preds = net.forward(X_test)\n",
    "        accuracy = (test_preds.argmax(dim=1) == y_test).float().mean().data\n",
    "        \n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один шаг обучения сети на конкретном батче называется итерацией (`iteration`). В рамках одной эпохи обучение на батчах производится в несколько итераций.<br>\n",
    "Эффект накопления градиентов в `PyTorch` работает так: старые градиенты весов и сдвигов нейронной сети после обратного распространения ошибки не заменяются на новые, а суммируются с ними. Эту особенность `PyTorch` используют для ускорения обучения: можно делать шаг алгоритма оптимизации не для каждого батча, а для нескольких батчей, например $5$ или $10$. <i>Но если функция потерь для батча вычисляется как среднее арифметическое по всем его элементам (поведение по умолчанию), нужно усреднить значение этой функции</i>. Так выглядит обучение с накоплением градиентов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "accumulation_iteration = 5 # делать оптимизационный шаг каждый 5-й батч\n",
    "\n",
    "num_batches = ceil(len(X_train)/batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # случайная перестановка объектов\n",
    "    order = np.random.permutation(len(X_train))\n",
    "    optimizer.zero_grad()\n",
    "    for batch_i in range(num_batches):\n",
    "        start_index = batch_i * batch_size\n",
    "        \n",
    "        # получение индексов текущего батча\n",
    "        batch_indexes = order[start_index:start_index+batch_size]\n",
    "        X_batch = X_train[batch_indexes]\n",
    "        y_batch = y_train[batch_indexes\n",
    "    \n",
    "        preds = net.forward(X_batch) \n",
    "                \n",
    "        loss_value = loss(preds, y_batch) / accumulation_iteration\n",
    "        loss_value.backward()\n",
    "                \n",
    "        if ((batch_i + 1) % accumulation_iteration == 0) or (batch_i + 1 == num_batches):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        net.eval()\n",
    "        test_preds = net.forward(X_test)\n",
    "        accuracy = (test_preds.argmax(dim=1) == y_test).float().mean().data\n",
    "        \n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = 'blue'>Задача обучения нейронной сети</font>\n",
    "Спрогнозируйте стабильность электрических сетей. Вам предоставили файл с данными:\n",
    " '/datasets/Electrical_Grid_Stability.csv'.\n",
    "Признаки обезличены: по названиям вы никак не сможете понять, что они значат. Целевая переменная `stability` отвечает за стабильность сети: если её значение — '1', то сеть стабильна, '0' — нет.\n",
    "Начните работу над задачей: подготовьте данные и инициализируйте нейронную сеть.\n",
    "С помощью метода `train_test_split` разделите данные на обучающие и тестовые. Тестовые данные должны составлять $0.3$ от размера датасета, используйте параметр `test_size`. Укажите параметр `shuffle=True` для того, чтобы перемешать датасет перед разделением. Для воспроизведения результатов укажите параметр `random_state=42`.<br>\n",
    "Инициализируйте нейронную сеть прямого распространения с двумя скрытыми слоями: \n",
    "<br>$12$ входных нейронов, \n",
    "<br>$8$ нейронов в первом скрытом слое, \n",
    "<br>$4$ нейрона во втором скрытом слое и один нейрон в выходном слое.\n",
    "<br>После первого скрытого слоя примените функцию активации `ReLU`, после второго — `гиперболический тангенс`, после выходного — `сигмоиду`.\n",
    "<br>Постройте цикл обучения нейронной сети, которая будет прогнозировать стабильность электрических цепей.\n",
    "Инициализируйте оптимизатор `Adam` с параметром шага $lr=1e-3$.\n",
    "<br>Объявите функцию потерь бинарной кросс-энтропии.\n",
    "<br>Дополните цикл обучения сети. Каждую десятую эпоху и в последнюю эпоху обучения проверяйте качество на тестовых данных. <br>В качестве метрики качества используйте долю правильных ответов.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "data = pd.read_csv('/datasets/Electrical_Grid_Stability.csv', sep=';')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(columns=['stability']), \n",
    "    data.stability, \n",
    "    test_size=0.3, \n",
    "    shuffle=True)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train.values)\n",
    "X_test = torch.FloatTensor(X_test.values)\n",
    "y_train = torch.FloatTensor(y_train.values)\n",
    "y_test = torch.FloatTensor(y_test.values)\n",
    "\n",
    "\n",
    "n_in_neurons = 12\n",
    "n_hidden_neurons_1 = 8\n",
    "n_hidden_neurons_2 = 4\n",
    "n_out_neurons = 1 \n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(n_in_neurons, n_hidden_neurons_1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(n_hidden_neurons_1, n_hidden_neurons_2),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(n_hidden_neurons_2, n_out_neurons), \n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "num_batches = ceil(len(X_train)/batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\torder = np.random.permutation(len(X_train)) # создайте случайную перестановку индексов объектов\n",
    "\tfor batch_idx in range(num_batches):\n",
    "\t\tstart_index = batch_idx * batch_size# посчитайте номер стартового объекта батча\n",
    "\t\toptimizer.zero_grad()\n",
    "  \n",
    "\t\tbatch_indexes = order[start_index:start_index+batch_size] # извлеките индексы объектов текущего обатча\n",
    "\t\tX_batch = X_train[batch_indexes]\n",
    "\t\ty_batch = y_train[batch_indexes]\n",
    "  \n",
    "\t\tpreds = net.forward(X_batch).flatten()\n",
    "\t        \n",
    "\t\tloss_value = loss(preds, y_batch)\n",
    "\n",
    "\t\tloss_value.backward()\n",
    "\t        \n",
    "\t\toptimizer.step()\n",
    "\t\t\n",
    "\tif epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "\t\tnet.eval()\n",
    "\t\ttest_preds = net.forward(X_test)\n",
    "\t\taccuracy = (torch.round(test_preds) == y_test).float().mean().data\n",
    "\t\tprint(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод\n",
    "<br>tensor(0.6463)\n",
    "<br>tensor(0.6081)\n",
    "<br>tensor(0.5661)\n",
    "<br>tensor(0.5615)\n",
    "<br>tensor(0.5467)\n",
    "<br>tensor(0.5480)\n",
    "<br>tensor(0.5419)\n",
    "<br>tensor(0.5455)\n",
    "<br>tensor(0.5404)\n",
    "<br>tensor(0.5503)\n",
    "<br>tensor(0.5417)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициализация параметров сети\n",
    "От того, как вы инициализируете сеть, зависит, как быстро функция потерь придёт к оптимальному значению.<br>\n",
    "Нейронная сеть в `PyTorch` создаётся либо как наследник `nn.Module`, либо с помощью `Sequential`\n",
    "## Инициализация равномерным распределением `nn.Module`\n",
    "В PyTorch методы инициализации весов находятся в модуле `torch.nn.init`. Если выбрана инициализация равномерным распределением, нужно использовать метод `uniform_()` и применить его ко всем параметрам, указав границы распределения `a` и `b` — $0$ и $1$ по умолчанию. Для доступа к параметрам полносвязного слоя используйте атрибуты `.weigth` и `.bias` — веса и отступы:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in_neurons, n_hidden_neurons, n_out_neurons):\n",
    "      super(Net, self).__init__()\n",
    "            \n",
    "      self.fc1 = nn.Linear(n_in_neurons, n_hidden_neurons)\n",
    "      self.act1 = nn.Sigmoid()\n",
    "      self.fc2 = nn.Linear(n_hidden_neurons, n_out_neurons)        \n",
    "      self.act2 = nn.ReLU()\n",
    "            \n",
    "      nn.init.uniform_(self.fc1.weight, a=-1, b=2)\n",
    "      nn.init.uniform_(self.fc1.bias, a=1, b=2)\n",
    "      nn.init.uniform_(self.fc2.weight, b=3)\n",
    "      nn.init.uniform_(self.fc2.bias, a=-1)\n",
    "   \n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.fc1(x)\n",
    "      x = self.act1(x)\n",
    "      x = self.fc2(x)\n",
    "      x = self.act2(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация нормальным распределением `Sequential`\n",
    "Теперь выберем сеть, созданную с помощью `Sequential`, и инициализируем её нормальным распределением. Для этого распределения используется метод `normal_`. Параметры метода: `mean` — медиана и `std` — стандартное отклонение. \n",
    "Сначала напишите метод, который будет инициализировать слой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    if type(layer) == nn.Linear: # Проверка, что слой — полносвязный\n",
    "        nn.init.normal_(layer.weight, mean=0.5, std=0.5)\n",
    "        nn.init.normal_(layer.bias, mean=-0.5, std=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем, примените его ко всем слоям сети с помощью метода `apply`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(n_in_neurons, n_hidden_neurons),\n",
    "    nn.Sigmoid(), \n",
    "    nn.Linear(n_hidden_neurons, n_out_neurons), \n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось пояснить, как выбирать параметры распределений для инициализации. Этот выбор зависит от того, насколько функция активации сети симметрична относительно $0$.\n",
    "## Выбор параметров при симметричной функции активации\n",
    "способ называют в честь одного из авторов статьи инициализацией Ксавье (Xavier initialization) или инициализацией Глоро (Glorot initialization). Чтобы использовать этот метод в PyTorch, вызовите метод `xavier_uniform_` или `xavier_normal_`. Инициализацию Ксавье в PyTorch можно применять только к весам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    if type(layer) == nn.Linear: # Проверка, что слой – полносвязный\n",
    "        nn.init.xavier_uniform_(layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор параметров в случае, если функция активации несимметрична\n",
    "Это называется инициализацией Кайминга (Kaiming initialization) или инициализацией Хе (He initialization). Чтобы использовать этот метод в PyTorch, вызовите метод `kaiming_uniform_` или `kaiming_normal_`. У обоих методов есть параметры, которые нужно использовать. Во-первых, нужно указать функцию активации `ReLU`: `nonlinearity='relu'`. Во-вторых, нужно указать в параметре `mode`, в каком случае нужно, чтобы дисперсия весов сохранялась: при прямом `('fan_in')` или при обратном `('fan_out')` распространении. Инициализацию Кайминга в PyTorch можно применять только к весам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    if type(layer) == nn.Linear: # Проверка, что слой – полносвязный\n",
    "        nn.init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию в PyTorch веса и смещения инициализируются случайными числами из равномерного распределения с границами<br>\n",
    "$$\n",
    "a = - \\sqrt{\\frac{1}{n_{i-1}}}, b = \\sqrt{\\frac{1}{n_{i-1}}}\n",
    "$$\n",
    "Удачная инициализация весов помогает быстрее найти минимум функции, а в некоторых случаях предотвращает попадание в неподходящий локальный минимум и повышает качество модели.\n",
    "Инициализация весов — это не единственный способ улучшить нейронную сеть.\n",
    "### Задача 1\n",
    "\n",
    "Инициализируйте нейронную сеть прямого распространения с двумя скрытыми слоями слоем, состоящую из 12 входных нейронов, 8 нейронов в первом скрытом слое, 4  нейронов во втором скрытом слое и <u>одного</u> нейрона в выходном слое.\n",
    "<br>После первого скрытого слоя примените функцию активации `гиперболический тангенс`, после второго скрытого — `ReLU`, после выходного слоя — `сигмоиду`.\n",
    "Инициализируйте веса и смещения:\n",
    "<br>В первом слое равномерным распределением от −2 до 2.\n",
    "<br>Во втором слое веса — с помощью инициализации Кайминга, смещения — нормальным распределением с математическим ожиданием 0.5 и среднеквадратичным отклонением 0.7.\n",
    "<br>В выходном слое веса — с помощью нормальной инициализации `Ксавье`, смещения — `нормальным распределением с математическим ожиданием ` 0.5 и `среднеквадратичным отклонением` 0.7.\n",
    "<br>Инициализируйте оптимизатор `Adam` с параметром шага $lr=1e-3$.\n",
    "<br>Объявите функцию потерь `бинарной кросс-энтропии`.\n",
    "<br>Дополните цикл обучения сети. Каждую 10-ю эпоху и в последнюю эпоху обучения проверяйте качество на тестовых данных. В качестве метрики качества используйте долю правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in_neurons, n_hidden_neurons_1,\n",
    "                 n_hidden_neurons_2, n_out_neurons):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_in_neurons, n_hidden_neurons_1)\n",
    "        self.act1 = nn.Tanh()\n",
    "\n",
    "        self.fc2 = nn.Linear(n_hidden_neurons_1, n_hidden_neurons_2)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(n_hidden_neurons_2, n_out_neurons)\n",
    "        self.act3 = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        nn.init.uniform_(self.fc1.weight, a=-2, b=2) #равномерным распределением от −2 до 2.\n",
    "        nn.init.uniform_(self.fc1.bias, a=-2, b=2)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu') #с помощью инициализации Кайминга\n",
    "        nn.init.normal_(self.fc2.bias, mean=0.5, std=0.7) #с помощью инициализации Кайминга\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.normal_(self.fc3.bias, mean=0.5, std=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.act3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "data = pd.read_csv('/datasets/Electrical_Grid_Stability.csv', sep=';')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(columns=['stability']),\n",
    "    data.stability,\n",
    "    test_size=0.3,\n",
    "    shuffle=True)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train.values)\n",
    "X_test = torch.FloatTensor(X_test.values)\n",
    "y_train = torch.FloatTensor(y_train.values)\n",
    "y_test = torch.FloatTensor(y_test.values)\n",
    "\n",
    "\n",
    "n_in_neurons = 12\n",
    "n_hidden_neurons_1 = 8\n",
    "n_hidden_neurons_2 = 4\n",
    "n_out_neurons = 1\n",
    "\n",
    "net = Net(n_in_neurons\n",
    "          , n_hidden_neurons_1\n",
    "          , n_hidden_neurons_2\n",
    "          , n_out_neurons) #nn.Sequential\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3) ###&&&&\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    order = np.random.permutation(len(X_train)) # создайте случайную перестановку индексов объектов\n",
    "    preds = net.forward(X_train).flatten()\n",
    "    loss_value = loss(preds, y_train)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        net.eval()\n",
    "        test_preds = net.forward(X_test)\n",
    "        accuracy = (test_preds.argmax(dim=1) == y_test).float().mean().data\n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><font color = 'blue'>Создайте полносвязную нейронную сеть с произвольным числом скрытых слоёв</font></u>**. Количество нейронов в каждом слое задано в списке `n_neurons`. Длина списка не меньше двух. Каждый элемент списка является числом нейронов в соответствующих слоях. В качестве функции активации для нечётных слоёв используйте `nn.ReLU()`, для чётных — гиперболический тангенс `nn.Tanh()`, входной слой считается нулевым. Для выходного слоя используйте функцию активации `nn.Sigmoid()`. \n",
    "<br>Создайте метод `init_weights` для инициализации полносвязных слоёв. Инициализируйте веса с помощью нормального распределения с математическим ожиданием 0.5 и среднеквадратичным отклонением 2, а смещения — с помощью нормального −0.5 и среднеквадратичным отклонением 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "data = pd.read_csv('/datasets/Electrical_Grid_Stability.csv', sep=';')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(columns=['stability']),\n",
    "    data.stability,\n",
    "    test_size=0.3,\n",
    "    shuffle=True)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train.values)\n",
    "X_test = torch.FloatTensor(X_test.values)\n",
    "y_train = torch.FloatTensor(y_train.values)\n",
    "y_test = torch.FloatTensor(y_test.values)\n",
    "\n",
    "n_neurons = [12, 9, 6, 3, 1]\n",
    "net_layers = []\n",
    "\n",
    "for i in range(1, len(n_neurons) - 1):\n",
    "    #net_layers.append(...)\n",
    "    net_layers.append(nn.Linear(n_neurons[i-1], n_neurons[i]))\n",
    "    if (i+1) % 2 == 0:\n",
    "        net_layers.append(nn.Tanh())\n",
    "    else:\n",
    "        net_layers.append(nn.ReLU())\n",
    "\n",
    "net_layers.append(nn.Linear(n_neurons[-2], n_neurons[-1]))\n",
    "net_layers.append(nn.Sigmoid())\n",
    "\n",
    "net = nn.Sequential(*net_layers) # такая запись позволяет передавать элементы списка как параметры для инициализации\n",
    "\n",
    "\n",
    "def init_weights(layer):\n",
    "    if type(layer) == nn.Linear: # Проверка, что слой – полносвязный\n",
    "        nn.init.normal_(layer.weight, mean=0.5, std=2)\n",
    "        nn.init.normal_(layer.bias, mean=-0.5, std=1)\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    preds = net.forward(X_train).flatten()\n",
    "\n",
    "    loss_value = loss(preds, y_train)\n",
    "    # print(loss_value)\n",
    "\n",
    "    loss_value.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        net.eval()\n",
    "        test_preds = net.forward(X_test)\n",
    "        accuracy = (torch.round(test_preds) == y_test).float().mean().data\n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регуляризация весов\n",
    "Регуляризация весов — это подход, при котором модель штрафуется за большие значение весов. Есть два основных способа начислять штраф: `L1` и `L2` регуляризация. Числа означают степень, в которую будут возводить веса. **`L1` — взятие модуля, а `L2` — возведение в квадрат**.<br>\n",
    "Pегуляризация предотвращает переобучение, ограничивая значения весов, a также помогает решать проблему взрыва градиента<br>\n",
    "Регуляризация весов — дополнительное слагаемое к функции потерь. Вместе с ним общая функция потерь Loss имеет вид:\n",
    "$$\n",
    "Loss = Error(y, \\hat{y})+λ*Loss_{reg}\n",
    "$$\n",
    "<br>Где:\n",
    " <br>$Error(y, \\hat{y})$ - обычная функция потерь (например, кросс-энтропия для классификации);\n",
    " <br>$λ$-весовой коэффициент, или лямбда, со значением порядка одной сотой или тысячной. Лямбда — это гиперпараметр, который подбирается для конкретной задачи;\n",
    "<br>$Loss_{reg}$— регуляризационная составляющая.\n",
    "\n",
    "## `L1` регуляризация\n",
    "`L1` регуляризацию легко добавить на PyTorch. Пусть у вас есть модель `model` и  функция потерь `loss`:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "l1_lambda = 0.001\n",
    "l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "result_loss = loss + l1_lambda * l1_norm\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `L2` регуляризация\n",
    "`L2` регуляризация — это частный случай регуляризации по Тихонову. Она очень похожа на `L1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "l2_lambda = 0.001\n",
    "l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "result_loss = loss + l2_lambda * l2_norm\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, для регуляризации можно одновременно использовать и `L1` и `L2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "l1_lambda = 0.0005\n",
    "l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "l2_lambda = 0.001\n",
    "l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "result_loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения лямбды не всегда одинаковы. Вам предстоит подбирать их под условия каждой задачи.<br>\n",
    "## Применимость регуляризации весов\n",
    "До недавнего времени регуляризация весов применялась повсеместно и была почти обязательной. Сейчас же она нужна крайне редко, в том числе из-за `Batch Normalization`. Кроме того, регуляризация весов если и увеличивает метрики, то незначительно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "Batch Normalization, или BatchNorm, — это метод регуляризации и стабилизации обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 50)\n",
    "        self.layer2 = nn.Linear(50, 200)\n",
    "        self.layer3 = nn.Linear(200, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим к ней BatchNorm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.layer2 = nn.Linear(50, 200)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.layer3 = nn.Linear(200, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До сих пор нет консенсуса о том, куда именно лучше добавить Batch Normalization: до или после активации<br>\n",
    "Чаще всего нормализация работает в обоих случаях.<br>\n",
    "Batch Normalization — это распространённый и эффективный метод регуляризации и стабилизации обучения с помощью нормализации батча. В популярных фреймворках его легко добавить в виде дополнительного слоя сети.\n",
    "## Dropout\n",
    "Она основана на очень простой, но рабочей идее: «выключить» часть нейронов сети.<br>\n",
    "Единственное, что можно настроить при использовании Dropout, — это доля «выключенных» нейронов $p$. Если применить Dropout c гиперпараметром $p=0.25$ к слою из 128 нейронов, то перестанет работать четверть нейронов: 32 (128/4). «Включённым» или «выключенным» может быть только целый нейрон. Если при делении числа нейронов на $p$ получается дробь, то она округлится по принятым во фреймворке правилам.<br>\n",
    "У нейрона нет рубильника. Чтобы его «выключить», нужно поменять значения выходов нейрона на нулевые. Для этого их нужно умножить на матрицу из нулей и единиц. Число нулей равняется количеству «выключенных», в котором случайные $p$ элементов нули, а остальные — единицы.<br>\n",
    "При использовании Dropout нейроны не «выключаются» раз и навсегда. На каждом шаге обучения не работают случайные нейроны: любой из них на текущей итерации может иметь значение ноль, а на следующей принять своё исходное.<br>\n",
    "Пример сети с Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 50)\n",
    "        self.dp1 = nn.Dropout(p=0.2)\n",
    "        self.layer2 = nn.Linear(50, 200)\n",
    "        self.dp2 = nn.Dropout(p=0.5)\n",
    "        self.layer3 = nn.Linear(200, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dp1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество слоёв Dropout и их значения $p$ — это гиперпараметры, которые можно подгонять под задачу. Кроме того, Dropout можно применять к любому тензору без контекста обучения. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Dropout(p=0.5)\n",
    "input_ = torch.randn(2, 10)\n",
    "print(m(input_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что всего было обнулено десять значений из двадцати, но не по пять в каждой строке, а четыре в первой и шесть во второй. То есть вероятность обнуления применяется ко всему тензору, а не к отдельным строкам или столбцам\n",
    "### Когда и как применять\n",
    "Его применяют, когда диагностировали переобучение, то есть сеть отлично работает с тренировочными данными и плохо с тестовыми.<br>\n",
    "Dropout чаще всего применяют к полносвязным слоям сети. Его используют и со свёрточными и рекуррентными слоями, однако это далеко не всегда оправдано.<br>\n",
    "Также довольно редко применяют BatchNorm и Dropout вместе. Как правило, BatchNorm используют чаще. <br>\n",
    "### Model.eval() метод\n",
    "В Pytorch у модели есть два метода: `train()` и `eval()`. Один метод используется для фазы обучения, другой для фазы предсказания. Но что именно делает `eval()`? Он меняет поведение слоёв `BatchNorm` и `Dropout`. При `eval()` в `BatchNorm` статистики батча не считаются, а используются аккумулированные и подсчитанные ранее. А `Dropout` слой перестаёт работать, то есть нейроны не «выключаются» ($p=0$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sum_{i=0}^n \\pi^2 = \\Delta\\frac{(n^2+\\epsilon)(2n+1)}{6\\phi}$<br>\n",
    "$\\sum_{i=0}^n \\pi^2 = \\frac{(n^2+\\epsilon)(2n+1)}{6\\phi}$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
